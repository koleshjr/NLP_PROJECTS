{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from typing import Optional, Union\n",
    "import pandas as pd, numpy as np, torch\n",
    "from datasets import dataset\n",
    "from dataclasses import dataclass \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForMultipleChoice \n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cfg:\n",
    "    use_peft = False \n",
    "    freeze_layers = 18\n",
    "    freeze_embeddings = True\n",
    "    max_input = 256\n",
    "    model = 'microsoft/deberta-v3-large'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Your Dataset\n",
    "* Should have the columns: prompt, context, A, B, C, D, E, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"Your path to train.csv\")\n",
    "df_valid = pd.read_csv(\"Your path to val.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Cfg.model)\n",
    "\n",
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {idx: option for option, idx in option_to_index.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    first_sentence = [\"[CLS] \" + example['context'] ] * 5\n",
    "    second_sentence = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP] \" for option in 'ABCDE']\n",
    "    tokenized_examples = tokenizer(first_sentence, second_sentence, truncation=\"only_first\", max_length=Cfg.max_input, padding='max_length', add_special_tokens=False)\n",
    "    tokenized_examples[\"labels\"] = [option_to_index[example[\"answer\"]]]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "dataset_valid = dataset.Dataset.from_pandas(df_valid)\n",
    "dataset_train = dataset.Dataset.from_pandas(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the dataset\n",
    "tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_dataset_train = dataset_train.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(Cfg.model)\n",
    "\n",
    "if Cfg.use_peft:\n",
    "    print('Using PEFT')\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha = 4,\n",
    "        task_type = TaskType.SEQ_CLS, \n",
    "        lora_dropout = 0.1,\n",
    "        bias = \"none\",\n",
    "        inference_mode = False,\n",
    "        target_modules = [\"query_proj\", \"value_proj\"],\n",
    "        modules_to_save = [\"classifier\", \"pooler\"]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "if Cfg.freeze_embeddings:\n",
    "    print('Freezing Embeddings')\n",
    "    from param in model.deberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    \n",
    "if Cfg.freeze_layers:\n",
    "    print('Freezing Layers')\n",
    "    for layer in model.deberta.encoder.layer[:Cfg.freeze_layers]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
