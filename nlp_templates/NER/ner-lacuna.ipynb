{"cells":[{"cell_type":"markdown","metadata":{},"source":["### NER : A TOKEN CLASSIFICATION APPROACH"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:49:13.325564Z","iopub.status.busy":"2023-12-20T14:49:13.325245Z","iopub.status.idle":"2023-12-20T14:49:27.315636Z","shell.execute_reply":"2023-12-20T14:49:27.314473Z","shell.execute_reply.started":"2023-12-20T14:49:13.325535Z"},"trusted":true},"outputs":[],"source":["! pip install datasets transformers seqeval\n","! pip install -u accelerate"]},{"cell_type":"markdown","metadata":{},"source":["### Import Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:49:27.317721Z","iopub.status.busy":"2023-12-20T14:49:27.317169Z","iopub.status.idle":"2023-12-20T14:49:34.303012Z","shell.execute_reply":"2023-12-20T14:49:34.301993Z","shell.execute_reply.started":"2023-12-20T14:49:27.317689Z"},"trusted":true},"outputs":[],"source":["import transformers\n","import torch\n","import pandas as pd\n","import numpy as np\n","from datasets import Dataset, DatasetDict, load_dataset, load_metric\n","import ast\n","import wandb\n","import random\n","import os\n","import warnings\n","import glob\n","from sklearn.metrics import accuracy_score\n","warnings.filterwarnings(\"ignore\")\n","from transformers import (\n","     AutoTokenizer, \n","     Trainer, TrainingArguments,\n","    EarlyStoppingCallback, AutoModelForTokenClassification,\n",")\n","print(transformers.__version__)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Configs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:49:34.305914Z","iopub.status.busy":"2023-12-20T14:49:34.305339Z","iopub.status.idle":"2023-12-20T14:49:34.313585Z","shell.execute_reply":"2023-12-20T14:49:34.312624Z","shell.execute_reply.started":"2023-12-20T14:49:34.305885Z"},"trusted":true},"outputs":[],"source":["class CFG :\n","    path = '/kaggle/input/masakhane-ner-update/'\n","    project_name = 'Ner_Masakhane_POS_Classification'\n","    model_nm = 'Davlan/afro-xlmr-large-75L'\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    seed = 42\n","    test_path = \"/kaggle/working/masakhane-pos/data/Test.csv\"\n","    train_path = \"/kaggle/working/masakhane-pos/data/africa_lan.csv\"\n","    label_mappings = {0: 'ADJ', 1: 'ADP', 2: 'ADV', 3: 'AUX', 4: 'CCONJ', 5: 'DET', 6: 'INTJ', 7: 'NOUN', 8: 'NUM', 9: 'PART', 10: 'PRON', 11: 'PROPN', 12: 'PUNCT', 13: 'SCONJ', 14: 'SYM', 15: 'VERB', 16: 'X'}\n","    batch_size = 16\n","    max_length = 135\n","    num_classes = 17\n","    valid_languages = ['wol' ,'sna']\n","    dropout = 0.0\n","    num_epoch = 30\n","    early_stopping_patience = 30\n","    working_path = '/kaggle/working/'\n","    lr = 1e-06\n","    warmup_steps = 100\n","    gradient_accumulation_steps = 1\n","    data_dir = \"/kaggle/working/masakhane-pos/data\"\n"]},{"cell_type":"markdown","metadata":{},"source":["### Track Your Runs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:49:34.315094Z","iopub.status.busy":"2023-12-20T14:49:34.314812Z","iopub.status.idle":"2023-12-20T14:50:07.251994Z","shell.execute_reply":"2023-12-20T14:50:07.251019Z","shell.execute_reply.started":"2023-12-20T14:49:34.315068Z"},"trusted":true},"outputs":[],"source":["wandb.login()  \n","wandb.init(project=CFG.project_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:07.253538Z","iopub.status.busy":"2023-12-20T14:50:07.253196Z","iopub.status.idle":"2023-12-20T14:50:07.261296Z","shell.execute_reply":"2023-12-20T14:50:07.260211Z","shell.execute_reply.started":"2023-12-20T14:50:07.253505Z"},"trusted":true},"outputs":[],"source":["%env WANDB_LOG_MODEL=true   #log every trained model"]},{"cell_type":"markdown","metadata":{},"source":["### Reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:07.263081Z","iopub.status.busy":"2023-12-20T14:50:07.262743Z","iopub.status.idle":"2023-12-20T14:50:07.272403Z","shell.execute_reply":"2023-12-20T14:50:07.271554Z","shell.execute_reply.started":"2023-12-20T14:50:07.263050Z"},"trusted":true},"outputs":[],"source":["def set_random_seed(random_seed):\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n","\n","    torch.manual_seed(random_seed)\n","    torch.cuda.manual_seed(random_seed)\n","    torch.cuda.manual_seed_all(random_seed)\n","\n","    torch.backends.cudnn.deterministic = True\n","set_random_seed(CFG.seed)\n","transformers.set_seed(CFG.seed)"]},{"cell_type":"markdown","metadata":{},"source":["* add some telemetry - tells us which examples and software versions are being used"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:07.274111Z","iopub.status.busy":"2023-12-20T14:50:07.273839Z","iopub.status.idle":"2023-12-20T14:50:07.391357Z","shell.execute_reply":"2023-12-20T14:50:07.390491Z","shell.execute_reply.started":"2023-12-20T14:50:07.274086Z"},"trusted":true},"outputs":[],"source":["from transformers.utils import send_example_telemetry\n","send_example_telemetry(\"token_classification_notebook\", framework=\"pytorch\")"]},{"cell_type":"markdown","metadata":{},"source":["* Load the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:07.392813Z","iopub.status.busy":"2023-12-20T14:50:07.392543Z","iopub.status.idle":"2023-12-20T14:50:08.103123Z","shell.execute_reply":"2023-12-20T14:50:08.102100Z","shell.execute_reply.started":"2023-12-20T14:50:07.392788Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(CFG.train_path)\n","train = train.dropna()\n","test = pd.read_csv(CFG.test_path)\n","\n","display(train.head(), test.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:08.107757Z","iopub.status.busy":"2023-12-20T14:50:08.107427Z","iopub.status.idle":"2023-12-20T14:50:08.325061Z","shell.execute_reply":"2023-12-20T14:50:08.323809Z","shell.execute_reply.started":"2023-12-20T14:50:08.107727Z"},"trusted":true},"outputs":[],"source":["test['Pos'] = 'X'  # assumed for now\n","test['sentence_Id'] = test['Id'].apply(lambda x: x.split('_')[0])\n","test = test.groupby('sentence_Id').agg(list).reset_index()\n","test.head(2)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare Data\n","\n","* Inside the function, two empty lists are initialized: example_words and example_labels. These lists will store the words and corresponding labels from the file.\n","It opens the file specified by file_path for reading, assuming it is encoded in UTF-8.\n","\n","* The code then enters a loop to read the file line by line. For each line:\n","\n","    * a. It strips any leading or trailing whitespace using line.strip().\n","\n","    * b. It checks if the length of the line is less than 2 characters or if the line is just a newline character (\"\\n\"). If either of these conditions is met, it indicates the end of an example.\n","\n","    * c. If an example has been completed (i.e., words is not empty), it appends the collected words to the example_words list and the corresponding labels to the example_labels list.\n","\n","    * d. It resets the words and labels lists to prepare for the next example.\n","\n","* If the line does not meet the criteria in step 4b (i.e., it's part of an ongoing example), it splits the line into words and labels. It assumes that the line is formatted with space-separated words and labels. Words are stored in the words list, and labels are stored in the labels list.\n","\n","* It checks if the line has both words and labels (i.e., len(splits) > 1). If so, it extracts the last element (the label), removes the newline character (\"\\n\"), and appends it to the labels list. If the line has no label, it assigns the label \"O.\"\n","\n","* After processing all lines in the file, the code checks if there are any remaining words in the words list. If so, it appends them as the last example in the example_words and example_labels lists.\n","\n","* Finally, the function returns two lists: example_words, which contains lists of words for each example, and example_labels, which contains lists of corresponding labels.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:08.328857Z","iopub.status.busy":"2023-12-20T14:50:08.328236Z","iopub.status.idle":"2023-12-20T14:50:08.338582Z","shell.execute_reply":"2023-12-20T14:50:08.337425Z","shell.execute_reply.started":"2023-12-20T14:50:08.328814Z"},"trusted":true},"outputs":[],"source":["def read_examples_from_file(file_path):\n","    example_words = []\n","    example_labels = []\n","    with open(file_path, encoding=\"utf-8\") as f:\n","        words = []\n","        labels = []\n","        for line in f:\n","            line = line.strip()\n","            if len(line) < 2  or line == \"\\n\":\n","                if words:\n","                    example_words.append(words)\n","                    example_labels.append(labels)\n","\n","                    words = []\n","                    labels = []\n","            else:\n","                splits = line.split(\" \")\n","                words.append(splits[0])\n","                if len(splits) > 1:\n","                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n","                else:\n","                    # Examples could have no label for mode = \"test\"\n","                    labels.append(\"O\")\n","        if words:\n","            example_words.append(words)\n","            example_labels.append(labels)\n","    return example_words, example_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:08.353347Z","iopub.status.busy":"2023-12-20T14:50:08.352945Z","iopub.status.idle":"2023-12-20T14:50:09.373363Z","shell.execute_reply":"2023-12-20T14:50:09.372409Z","shell.execute_reply.started":"2023-12-20T14:50:08.353298Z"},"trusted":true},"outputs":[],"source":["train_words = []\n","train_labels = []\n","\n","valid_words = []\n","valid_labels = []\n","\n","for file_path in glob.glob(f\"{CFG.data_dir}/**/*.txt\"):\n","    words_list, labels_list = read_examples_from_file(file_path)\n","\n","    language = file_path.split(os.sep)[-2]\n","    if language in CFG.valid_languages:\n","        valid_words.extend(words_list)\n","        valid_labels.extend(labels_list)\n","    else:\n","        train_words.extend(words_list)\n","        train_labels.extend(labels_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:09.375036Z","iopub.status.busy":"2023-12-20T14:50:09.374634Z","iopub.status.idle":"2023-12-20T14:50:09.383529Z","shell.execute_reply":"2023-12-20T14:50:09.382376Z","shell.execute_reply.started":"2023-12-20T14:50:09.374996Z"},"trusted":true},"outputs":[],"source":["len(train_labels), len(train_words), len(valid_labels), len(valid_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:09.385425Z","iopub.status.busy":"2023-12-20T14:50:09.385069Z","iopub.status.idle":"2023-12-20T14:50:09.437847Z","shell.execute_reply":"2023-12-20T14:50:09.436875Z","shell.execute_reply.started":"2023-12-20T14:50:09.385391Z"},"trusted":true},"outputs":[],"source":["train = pd.DataFrame()\n","train['Word'] = train_words\n","train['Pos'] = train_labels\n","train.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:09.439428Z","iopub.status.busy":"2023-12-20T14:50:09.439068Z","iopub.status.idle":"2023-12-20T14:50:09.458225Z","shell.execute_reply":"2023-12-20T14:50:09.457118Z","shell.execute_reply.started":"2023-12-20T14:50:09.439398Z"},"trusted":true},"outputs":[],"source":["valid = pd.DataFrame()\n","valid['Word'] = valid_words\n","valid['Pos'] = valid_labels\n","valid.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:09.460006Z","iopub.status.busy":"2023-12-20T14:50:09.459654Z","iopub.status.idle":"2023-12-20T14:50:09.468461Z","shell.execute_reply":"2023-12-20T14:50:09.467552Z","shell.execute_reply.started":"2023-12-20T14:50:09.459978Z"},"trusted":true},"outputs":[],"source":["print(train.shape, valid.shape, test.shape)\n","\n","labels = [\"X\", \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\"]\n","CFG.num_classes = len(labels)"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenization\n","* The code starts by importing the AutoTokenizer class from the \"transformers\" library. This library is commonly used for working with pre-trained models for natural language processing.\n","\n","* It initializes a tokenizer object using the AutoTokenizer.from_pretrained method. The specific model used for tokenization is determined by the value of CFG.model_nm.\n","\n","* The convert_to_feature function takes several arguments, including row, which is expected to be a row of data containing words and their corresponding labels. It also takes various configuration parameters, such as max_seq_length, cls_token_at_end, and token-related settings.\n","\n","* Inside the function, a label_map is created, which is a dictionary that maps labels to their integer representations. This mapping is based on the label_list provided as an argument.\n","\n","* The function processes the words and labels in the input row. It tokenizes each word using the tokenizer and appends the resulting tokens to the tokens list. The corresponding label IDs are also appended to the label_ids list. If a word is tokenized into multiple sub-tokens, the label is repeated for each sub-token.\n","\n","* The code accounts for the maximum sequence length by truncating or padding the tokens and label_ids lists. If the length of tokens exceeds max_seq_length minus some special tokens count (which depends on the tokenizer and settings), it truncates the lists.\n","\n","* It adds special tokens like [SEP] (separator token) and [CLS] (classification token) to the tokens and label_ids lists as needed. The specific position of these tokens depends on the cls_token_at_end setting.\n","\n","* Segment IDs are created to distinguish between different segments in the input. For most models, sequence_a_segment_id is assigned to all tokens in the input.\n","\n","* The input_ids are generated by converting the tokens into their corresponding token IDs using the tokenizer.\n","\n","* An input_mask is created to distinguish between real tokens and padding tokens. Real tokens have a value of 1, while padding tokens have a value of 0.\n","\n","* The code ensures that the input data is zero-padded to the max_seq_length. The specific padding strategy depends on the pad_on_left setting.\n","\n","* Several assertions are used to verify the lengths of various lists to ensure they match the max_seq_length.\n","\n","* Finally, the function returns a dictionary containing the following keys: input_ids, input_mask, segment_ids, and label_ids, which represent the processed input features for a natural language processing model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:09.470700Z","iopub.status.busy":"2023-12-20T14:50:09.469921Z","iopub.status.idle":"2023-12-20T14:50:13.095393Z","shell.execute_reply":"2023-12-20T14:50:13.094401Z","shell.execute_reply.started":"2023-12-20T14:50:09.470665Z"},"trusted":true},"outputs":[],"source":["\n","tokenizer = AutoTokenizer.from_pretrained(CFG.model_nm, use_fast=True)\n","def convert_to_feature(\n","    row,\n","    tokenizer=tokenizer,\n","    label_list=labels,\n","    max_seq_length=CFG.max_length,\n","    cls_token_at_end=False,\n","    cls_token=\"[CLS]\",\n","    cls_token_segment_id=1,\n","    sep_token=\"[SEP]\",\n","    sep_token_extra=False,\n","    pad_on_left=False,\n","    pad_token=0,\n","    pad_token_segment_id=0,\n","    pad_token_label_id=-100,\n","    sequence_a_segment_id=0,\n","    mask_padding_with_zero=True,\n","):\n","    \"\"\" Loads a data file into a list of `InputBatch`s\n","        `cls_token_at_end` define the location of the CLS token:\n","            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n","            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n","        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n","    \"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    tokens = []\n","    label_ids = []\n","    for word, label in zip(row['Word'], row['Pos']):\n","        word_tokens = tokenizer.tokenize(word)\n","        tokens.extend(word_tokens)\n","        # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n","        label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n","\n","    # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n","    special_tokens_count = 3 if sep_token_extra else 2\n","    if len(tokens) > max_seq_length - special_tokens_count:\n","        tokens = tokens[: (max_seq_length - special_tokens_count)]\n","        label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n","\n","    tokens += [sep_token]\n","    label_ids += [pad_token_label_id]\n","    if sep_token_extra:\n","        # roberta uses an extra separator b/w pairs of sentences\n","        tokens += [sep_token]\n","        label_ids += [pad_token_label_id]\n","    segment_ids = [sequence_a_segment_id] * len(tokens)\n","\n","    if cls_token_at_end:\n","        tokens += [cls_token]\n","        label_ids += [pad_token_label_id]\n","        segment_ids += [cls_token_segment_id]\n","    else:\n","        tokens = [cls_token] + tokens\n","        label_ids = [pad_token_label_id] + label_ids\n","        segment_ids = [cls_token_segment_id] + segment_ids\n","\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","    # The mask has 1 for real tokens and 0 for padding tokens.\n","    # Only real tokens are attended to.\n","    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","    # Zero-pad up to the sequence length.\n","    padding_length = max_seq_length - len(input_ids)\n","    if pad_on_left:\n","        input_ids = ([pad_token] * padding_length) + input_ids\n","        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n","        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n","        label_ids = ([pad_token_label_id] * padding_length) + label_ids\n","    else:\n","        input_ids += [pad_token] * padding_length\n","        input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n","        segment_ids += [pad_token_segment_id] * padding_length\n","        label_ids += [pad_token_label_id] * padding_length\n","\n","    assert len(input_ids) == max_seq_length\n","    assert len(input_mask) == max_seq_length\n","    assert len(segment_ids) == max_seq_length\n","\n","    return dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:13.097179Z","iopub.status.busy":"2023-12-20T14:50:13.096863Z","iopub.status.idle":"2023-12-20T14:50:46.025054Z","shell.execute_reply":"2023-12-20T14:50:46.023824Z","shell.execute_reply.started":"2023-12-20T14:50:13.097142Z"},"trusted":true},"outputs":[],"source":["%%time\n","# train features\n","train_features = pd.DataFrame(train.apply(lambda row: convert_to_feature(row), axis=1).tolist())\n","train_features.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:46.027433Z","iopub.status.busy":"2023-12-20T14:50:46.026921Z","iopub.status.idle":"2023-12-20T14:50:50.762846Z","shell.execute_reply":"2023-12-20T14:50:50.761709Z","shell.execute_reply.started":"2023-12-20T14:50:46.027372Z"},"trusted":true},"outputs":[],"source":["%%time\n","# valid features\n","valid_features = pd.DataFrame(valid.apply(lambda row: convert_to_feature(row), axis=1).tolist())\n","valid_features.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:50.764391Z","iopub.status.busy":"2023-12-20T14:50:50.764067Z","iopub.status.idle":"2023-12-20T14:50:52.622704Z","shell.execute_reply":"2023-12-20T14:50:52.621416Z","shell.execute_reply.started":"2023-12-20T14:50:50.764337Z"},"trusted":true},"outputs":[],"source":["%%time\n","# test features\n","test_features = pd.DataFrame(test.apply(lambda row: convert_to_feature(row), axis=1).tolist())\n","test_features.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:52.624723Z","iopub.status.busy":"2023-12-20T14:50:52.624270Z","iopub.status.idle":"2023-12-20T14:50:54.157002Z","shell.execute_reply":"2023-12-20T14:50:54.155130Z","shell.execute_reply.started":"2023-12-20T14:50:52.624690Z"},"trusted":true},"outputs":[],"source":["masakhane = DatasetDict({\n","    \"train\": Dataset.from_pandas(train_features),\n","    \"valid\": Dataset.from_pandas(valid_features),\n","    \"test\": Dataset.from_pandas(test_features),\n","})\n","\n","masakhane"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:54.158647Z","iopub.status.busy":"2023-12-20T14:50:54.158296Z","iopub.status.idle":"2023-12-20T14:50:54.166785Z","shell.execute_reply":"2023-12-20T14:50:54.165937Z","shell.execute_reply.started":"2023-12-20T14:50:54.158610Z"},"trusted":true},"outputs":[],"source":["label_map = {i: label for i, label in enumerate(labels)}"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:54.168565Z","iopub.status.busy":"2023-12-20T14:50:54.168165Z","iopub.status.idle":"2023-12-20T14:50:54.180314Z","shell.execute_reply":"2023-12-20T14:50:54.179391Z","shell.execute_reply.started":"2023-12-20T14:50:54.168524Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(\n","    eval_pred,\n","    pad_token_label_id=-100,\n","):\n","    predictions, labels = eval_pred\n","    preds = np.argmax(predictions, axis = -1)\n","\n","    out_label_list = []\n","    preds_list = []\n","    for i in range(labels.shape[0]):\n","        for j in range(labels.shape[1]):\n","            if labels[i, j] != pad_token_label_id:\n","                out_label_list.append(label_map[labels[i][j]])\n","                preds_list.append(label_map[preds[i][j]])\n","\n","    accuracy = accuracy_score(out_label_list, preds_list)\n","    return {\"accuracy\": accuracy}"]},{"cell_type":"markdown","metadata":{},"source":["### Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:50:54.182529Z","iopub.status.busy":"2023-12-20T14:50:54.182160Z","iopub.status.idle":"2023-12-20T14:53:54.491040Z","shell.execute_reply":"2023-12-20T14:53:54.490037Z","shell.execute_reply.started":"2023-12-20T14:50:54.182498Z"},"trusted":true},"outputs":[],"source":["\n","model = AutoModelForTokenClassification.from_pretrained(\n","    CFG.model_nm,\n","    num_labels = CFG.num_classes,\n","    id2label={str(i): label for i, label in enumerate(labels)},\n","    label2id={label: i for i, label in enumerate(labels)},\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:53:54.492571Z","iopub.status.busy":"2023-12-20T14:53:54.492271Z","iopub.status.idle":"2023-12-20T14:53:54.500582Z","shell.execute_reply":"2023-12-20T14:53:54.499635Z","shell.execute_reply.started":"2023-12-20T14:53:54.492544Z"},"trusted":true},"outputs":[],"source":["epoch_steps = int(np.ceil(len(masakhane['train']) / CFG.batch_size))\n","display(epoch_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:53:54.502830Z","iopub.status.busy":"2023-12-20T14:53:54.501978Z","iopub.status.idle":"2023-12-20T14:53:54.512811Z","shell.execute_reply":"2023-12-20T14:53:54.511829Z","shell.execute_reply.started":"2023-12-20T14:53:54.502803Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='./masakhane-pos',\n","    evaluation_strategy = \"steps\",\n","    save_strategy='steps',\n","    eval_steps = epoch_steps//2,\n","    save_steps = epoch_steps//2,\n","    logging_strategy=\"steps\",\n","    logging_steps= epoch_steps//2,\n","    learning_rate=CFG.lr,\n","    save_total_limit=1,\n","    num_train_epochs=CFG.num_epoch,\n","    fp16=True,\n","    report_to='none',\n","    metric_for_best_model = \"accuracy\",\n","    greater_is_better=True,\n","    load_best_model_at_end = True,\n","    per_device_train_batch_size=CFG.batch_size,\n","    per_device_eval_batch_size=CFG.batch_size,\n","    warmup_steps = CFG.warmup_steps,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:53:54.519654Z","iopub.status.busy":"2023-12-20T14:53:54.519358Z","iopub.status.idle":"2023-12-20T14:53:57.568818Z","shell.execute_reply":"2023-12-20T14:53:57.567711Z","shell.execute_reply.started":"2023-12-20T14:53:54.519618Z"},"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","    model = model,\n","    args = training_args,\n","    compute_metrics = compute_metrics,\n","    train_dataset = masakhane['train'],\n","    eval_dataset = masakhane['valid'],\n","    tokenizer = tokenizer,\n","    callbacks = [EarlyStoppingCallback(5)],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T14:53:57.570791Z","iopub.status.busy":"2023-12-20T14:53:57.570378Z"},"trusted":true},"outputs":[],"source":["%%time\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"markdown","metadata":{},"source":["### Save Your Model To Hugging Face Hub"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","\n","folder_path = \"/kaggle/working/masakhane-pos\"  # Replace with the path to your folder\n","\n","# Remove existing files (if you don't need them)\n","shutil.rmtree(folder_path, ignore_errors=True)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.push_to_hub()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer.push_to_hub(\"Koleshjr/masakhane-pos\")"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["loaded_model = AutoModelForTokenClassification.from_pretrained(\n","    \"Koleshjr/masakhane-pos\",\n","    num_labels = CFG.num_classes,\n","\n",")\n","\n","loaded_tokenizer = AutoTokenizer.from_pretrained(\n","    'Koleshjr/masakhane-pos'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del model, trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pad_token_label_id=-100\n","\n","test_args = TrainingArguments(\n","    output_dir= '/content/',\n","    do_train =False,\n","    do_predict = True,\n","    dataloader_drop_last = False\n",")\n","\n","trainer = Trainer(\n","    model = loaded_model,\n","    args = test_args,\n",")\n","\n","test_results = trainer.predict(masakhane['test'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["result = test_results.predictions.argmax(axis = -1)\n","\n","preds_list = [[] for _ in range(test_results.label_ids.shape[0])]\n","for i in range(test_results.label_ids.shape[0]):\n","    for j in range(test_results.label_ids.shape[1]):\n","        if test_results.label_ids[i, j] != pad_token_label_id:\n","            preds_list[i].append(label_map[result[i][j]])\n","\n","test['Pos'] = preds_list\n","test.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def print_rows_with_mismatched_lengths(df):\n","    for index, row in df.iterrows():\n","        if len(row['Id']) != len(row['Pos']):\n","            print(f\"Row {index}: 'Id' length = {len(row['Id'])}, 'Pos' length = {len(row['Pos'])} sentence_id: {row['sentence_Id']}\")\n","\n","print_rows_with_mismatched_lengths(test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# def add_nouns_to_match_lengths(df):\n","#     for index, row in df.iterrows():\n","#         id_length = len(row['Id'])\n","#         pos_length = len(row['Pos'])\n","#         if id_length > pos_length:\n","#             # Calculate the difference in lengths\n","#             diff = id_length - pos_length\n","#             # Add 'NOUN' to 'Pos' to match lengths\n","#             df.at[index, 'Pos'].extend(['NOUN'] * diff)\n","\n","# # Call the function to add 'NOUN' labels to match lengths\n","# add_nouns_to_match_lengths(test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = test[['Id', 'Pos']].explode(column=['Id', 'Pos'], ignore_index=True)\n","submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = submission.reset_index(drop=True)\n","submission.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.to_csv(\"experiment_one.csv\", index = False)\n","submission['Pos'].value_counts()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3514917,"sourceId":6130586,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
