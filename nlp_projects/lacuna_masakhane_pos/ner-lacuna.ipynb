{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6130586,"sourceType":"datasetVersion","datasetId":3514917}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Lacuna Masakhane Parts of Speech Classification Challenge\n* Part-of-speech (POS) tagging is a crucial step in natural language processing (NLP), as it allows algorithms to understand the grammatical structure and meaning of a text. This is especially important in creating the building blocks for preparing low-resource African languages for NLP tasks. The MaseakhaPOS dataset for 20 typologically diverse African languages, including benchmarks, was created with the help of Lacuna Fund to try and address this problem.\n\n* The objective of this challenge is to create a machine learning solution that correctly classifies 14 parts of speech for the unrelated Luo and Setswana languages. You will need to build one solution that applies to both languages, not two solutions, one for each language.\n\n* It is important that only one solution be built for both languages as this is a step in creating a solution that can be applied to many different languages, instead of having to create a model for each language.\n\n","metadata":{}},{"cell_type":"markdown","source":"### Installing necessary dependencies","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/NtemKenyor/masakhane-pos","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:49:12.326770Z","iopub.execute_input":"2023-12-20T14:49:12.327160Z","iopub.status.idle":"2023-12-20T14:49:13.323249Z","shell.execute_reply.started":"2023-12-20T14:49:12.327111Z","shell.execute_reply":"2023-12-20T14:49:13.322012Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: destination path 'masakhane-pos' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets transformers seqeval\n!pip install -u accelerate","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:49:13.325245Z","iopub.execute_input":"2023-12-20T14:49:13.325564Z","iopub.status.idle":"2023-12-20T14:49:27.315636Z","shell.execute_reply.started":"2023-12-20T14:49:13.325535Z","shell.execute_reply":"2023-12-20T14:49:27.314473Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\nRequirement already satisfied: seqeval in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n\nUsage:   \n  pip install [options] <requirement specifier> [package-index-options] ...\n  pip install [options] -r <requirements file> [package-index-options] ...\n  pip install [options] [-e] <vcs project url> ...\n  pip install [options] [-e] <local project path> ...\n  pip install [options] <archive url/path> ...\n\nno such option: -u\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Importing Necessary Dependencies","metadata":{}},{"cell_type":"code","source":"import transformers\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset, DatasetDict, load_dataset, load_metric\nimport ast\nimport wandb\nimport random\nimport os\nimport warnings\nimport glob\nfrom sklearn.metrics import accuracy_score\nwarnings.filterwarnings(\"ignore\")\nfrom transformers import (\n    AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup,\n    AutoModelForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling,\n    EarlyStoppingCallback, AutoModelForTokenClassification,\n)\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:49:27.317169Z","iopub.execute_input":"2023-12-20T14:49:27.317721Z","iopub.status.idle":"2023-12-20T14:49:34.303012Z","shell.execute_reply.started":"2023-12-20T14:49:27.317689Z","shell.execute_reply":"2023-12-20T14:49:34.301993Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"4.31.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"class CFG :\n    path = '/kaggle/input/masakhane-ner-update/'\n    project_name = 'Ner_Masakhane_POS_Classification'\n    model_nm = 'Davlan/afro-xlmr-large-75L'\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    seed = 42\n    test_path = \"/kaggle/working/masakhane-pos/data/Test.csv\"\n    train_path = \"/kaggle/working/masakhane-pos/data/africa_lan.csv\"\n    label_mappings = {0: 'ADJ', 1: 'ADP', 2: 'ADV', 3: 'AUX', 4: 'CCONJ', 5: 'DET', 6: 'INTJ', 7: 'NOUN', 8: 'NUM', 9: 'PART', 10: 'PRON', 11: 'PROPN', 12: 'PUNCT', 13: 'SCONJ', 14: 'SYM', 15: 'VERB', 16: 'X'}\n    batch_size = 16\n    max_length = 135\n    num_classes = 17\n    valid_languages = ['wol' ,'sna']\n    dropout = 0.0\n    num_epoch = 30\n    early_stopping_patience = 30\n    working_path = '/kaggle/working/'\n    lr = 1e-06\n    warmup_steps = 100\n    gradient_accumulation_steps = 1\n    data_dir = \"/kaggle/working/masakhane-pos/data\"\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:49:34.305339Z","iopub.execute_input":"2023-12-20T14:49:34.305914Z","iopub.status.idle":"2023-12-20T14:49:34.313585Z","shell.execute_reply.started":"2023-12-20T14:49:34.305885Z","shell.execute_reply":"2023-12-20T14:49:34.312624Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### WANDB - Track your trial runs","metadata":{}},{"cell_type":"code","source":"wandb.login()  \nwandb.init(project=CFG.project_name)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:49:34.314812Z","iopub.execute_input":"2023-12-20T14:49:34.315094Z","iopub.status.idle":"2023-12-20T14:50:07.251994Z","shell.execute_reply.started":"2023-12-20T14:49:34.315068Z","shell.execute_reply":"2023-12-20T14:50:07.251019Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoleshjr\u001b[0m (\u001b[33mteam-kolesh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231220_144936-y5jmd4ke</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/team-kolesh/Ner_Masakhane_POS_Classification/runs/y5jmd4ke' target=\"_blank\">pleasant-jazz-55</a></strong> to <a href='https://wandb.ai/team-kolesh/Ner_Masakhane_POS_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/team-kolesh/Ner_Masakhane_POS_Classification' target=\"_blank\">https://wandb.ai/team-kolesh/Ner_Masakhane_POS_Classification</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/team-kolesh/Ner_Masakhane_POS_Classification/runs/y5jmd4ke' target=\"_blank\">https://wandb.ai/team-kolesh/Ner_Masakhane_POS_Classification/runs/y5jmd4ke</a>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/team-kolesh/Ner_Masakhane_POS_Classification/runs/y5jmd4ke?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f35c3fe0910>"},"metadata":{}}]},{"cell_type":"code","source":"%env WANDB_LOG_MODEL=true   #log every trained model","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:07.253196Z","iopub.execute_input":"2023-12-20T14:50:07.253538Z","iopub.status.idle":"2023-12-20T14:50:07.261296Z","shell.execute_reply.started":"2023-12-20T14:50:07.253505Z","shell.execute_reply":"2023-12-20T14:50:07.260211Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"env: WANDB_LOG_MODEL=true   #log every trained model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Reproducibility","metadata":{}},{"cell_type":"code","source":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True\nset_random_seed(CFG.seed)\ntransformers.set_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:07.262743Z","iopub.execute_input":"2023-12-20T14:50:07.263081Z","iopub.status.idle":"2023-12-20T14:50:07.272403Z","shell.execute_reply.started":"2023-12-20T14:50:07.263050Z","shell.execute_reply":"2023-12-20T14:50:07.271554Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"* add some telemetry - tells us which examples and software versions are being used","metadata":{}},{"cell_type":"code","source":"from transformers.utils import send_example_telemetry\nsend_example_telemetry(\"token_classification_notebook\", framework=\"pytorch\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:07.273839Z","iopub.execute_input":"2023-12-20T14:50:07.274111Z","iopub.status.idle":"2023-12-20T14:50:07.391357Z","shell.execute_reply.started":"2023-12-20T14:50:07.274086Z","shell.execute_reply":"2023-12-20T14:50:07.390491Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Loading The Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(CFG.train_path)\ntrain = train.dropna()\ntest = pd.read_csv(CFG.test_path)\n\ndisplay(train.head(), test.head())","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:07.392543Z","iopub.execute_input":"2023-12-20T14:50:07.392813Z","iopub.status.idle":"2023-12-20T14:50:08.103123Z","shell.execute_reply.started":"2023-12-20T14:50:07.392788Z","shell.execute_reply":"2023-12-20T14:50:08.102100Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"      word   tag lang\n0       Do  VERB  pcm\n1  senator  NOUN  pcm\n2      tok  VERB  pcm\n3      dis   DET  pcm\n4      one   NUM  pcm","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>tag</th>\n      <th>lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do</td>\n      <td>VERB</td>\n      <td>pcm</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>senator</td>\n      <td>NOUN</td>\n      <td>pcm</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tok</td>\n      <td>VERB</td>\n      <td>pcm</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dis</td>\n      <td>DET</td>\n      <td>pcm</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>one</td>\n      <td>NUM</td>\n      <td>pcm</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"               Id  Word Language  Pos\n0  Id00qog2f11n_0    Ne      luo  NaN\n1  Id00qog2f11n_1  otim      luo  NaN\n2  Id00qog2f11n_2  penj      luo  NaN\n3  Id00qog2f11n_3     e      luo  NaN\n4  Id00qog2f11n_4  kind      luo  NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Word</th>\n      <th>Language</th>\n      <th>Pos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Id00qog2f11n_0</td>\n      <td>Ne</td>\n      <td>luo</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Id00qog2f11n_1</td>\n      <td>otim</td>\n      <td>luo</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Id00qog2f11n_2</td>\n      <td>penj</td>\n      <td>luo</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Id00qog2f11n_3</td>\n      <td>e</td>\n      <td>luo</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Id00qog2f11n_4</td>\n      <td>kind</td>\n      <td>luo</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test['Pos'] = 'X'  # assumed for now\ntest['sentence_Id'] = test['Id'].apply(lambda x: x.split('_')[0])\ntest = test.groupby('sentence_Id').agg(list).reset_index()\ntest.head(2)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:08.107427Z","iopub.execute_input":"2023-12-20T14:50:08.107757Z","iopub.status.idle":"2023-12-20T14:50:08.325061Z","shell.execute_reply.started":"2023-12-20T14:50:08.107727Z","shell.execute_reply":"2023-12-20T14:50:08.323809Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"    sentence_Id                                                 Id  \\\n0  Id00qog2f11n  [Id00qog2f11n_0, Id00qog2f11n_1, Id00qog2f11n_...   \n1  Id01lywjj7oz  [Id01lywjj7oz_0, Id01lywjj7oz_1, Id01lywjj7oz_...   \n\n                                                Word  \\\n0  [Ne, otim, penj, e, kind, Februar, tarik, 9, g...   \n1  [Sifuna, ne, ojiwo, jonyuol, kod, joma, moko, ...   \n\n                                            Language  \\\n0  [luo, luo, luo, luo, luo, luo, luo, luo, luo, ...   \n1  [luo, luo, luo, luo, luo, luo, luo, luo, luo, ...   \n\n                                                 Pos  \n0  [X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, ...  \n1  [X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_Id</th>\n      <th>Id</th>\n      <th>Word</th>\n      <th>Language</th>\n      <th>Pos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Id00qog2f11n</td>\n      <td>[Id00qog2f11n_0, Id00qog2f11n_1, Id00qog2f11n_...</td>\n      <td>[Ne, otim, penj, e, kind, Februar, tarik, 9, g...</td>\n      <td>[luo, luo, luo, luo, luo, luo, luo, luo, luo, ...</td>\n      <td>[X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Id01lywjj7oz</td>\n      <td>[Id01lywjj7oz_0, Id01lywjj7oz_1, Id01lywjj7oz_...</td>\n      <td>[Sifuna, ne, ojiwo, jonyuol, kod, joma, moko, ...</td>\n      <td>[luo, luo, luo, luo, luo, luo, luo, luo, luo, ...</td>\n      <td>[X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Data Preprocessing\n\n* Inside the function, two empty lists are initialized: example_words and example_labels. These lists will store the words and corresponding labels from the file.\n\n* It opens the file specified by file_path for reading, assuming it is encoded in UTF-8.\n\n* The code then enters a loop to read the file line by line. For each line:\n\n   * a. It strips any leading or trailing whitespace using line.strip().\n\n   * b. It checks if the length of the line is less than 2 characters or if the line is just a newline character (\"\\n\"). If either of these conditions is met, it indicates the end of an example.\n\n   * c. If an example has been completed (i.e., words is not empty), it appends the collected words to the example_words list and the corresponding labels to the example_labels list.\n\n   * d. It resets the words and labels lists to prepare for the next example.\n\n* If the line does not meet the criteria in step 4b (i.e., it's part of an ongoing example), it splits the line into words and labels. It assumes that the line is formatted with space-separated words and labels. Words are stored in the words list, and labels are stored in the labels list.\n\n* It checks if the line has both words and labels (i.e., len(splits) > 1). If so, it extracts the last element (the label), removes the newline character (\"\\n\"), and appends it to the labels list. If the line has no label, it assigns the label \"O.\"\n\n* After processing all lines in the file, the code checks if there are any remaining words in the words list. If so, it appends them as the last example in the example_words and example_labels lists.\n\n* Finally, the function returns two lists: example_words, which contains lists of words for each example, and example_labels, which contains lists of corresponding labels.","metadata":{}},{"cell_type":"code","source":"def read_examples_from_file(file_path):\n    example_words = []\n    example_labels = []\n    with open(file_path, encoding=\"utf-8\") as f:\n        words = []\n        labels = []\n        for line in f:\n            line = line.strip()\n            if len(line) < 2  or line == \"\\n\":\n                if words:\n                    example_words.append(words)\n                    example_labels.append(labels)\n\n                    words = []\n                    labels = []\n            else:\n                splits = line.split(\" \")\n                words.append(splits[0])\n                if len(splits) > 1:\n                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n                else:\n                    # Examples could have no label for mode = \"test\"\n                    labels.append(\"O\")\n        if words:\n            example_words.append(words)\n            example_labels.append(labels)\n    return example_words, example_labels","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:08.328236Z","iopub.execute_input":"2023-12-20T14:50:08.328857Z","iopub.status.idle":"2023-12-20T14:50:08.338582Z","shell.execute_reply.started":"2023-12-20T14:50:08.328814Z","shell.execute_reply":"2023-12-20T14:50:08.337425Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_words = []\ntrain_labels = []\n\nvalid_words = []\nvalid_labels = []\n\nfor file_path in glob.glob(f\"{CFG.data_dir}/**/*.txt\"):\n    words_list, labels_list = read_examples_from_file(file_path)\n\n    language = file_path.split(os.sep)[-2]\n    if language in CFG.valid_languages:\n        valid_words.extend(words_list)\n        valid_labels.extend(labels_list)\n    else:\n        train_words.extend(words_list)\n        train_labels.extend(labels_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:08.352945Z","iopub.execute_input":"2023-12-20T14:50:08.353347Z","iopub.status.idle":"2023-12-20T14:50:09.373363Z","shell.execute_reply.started":"2023-12-20T14:50:08.353298Z","shell.execute_reply":"2023-12-20T14:50:09.372409Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"len(train_labels), len(train_words), len(valid_labels), len(valid_words)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:09.374634Z","iopub.execute_input":"2023-12-20T14:50:09.375036Z","iopub.status.idle":"2023-12-20T14:50:09.383529Z","shell.execute_reply.started":"2023-12-20T14:50:09.374996Z","shell.execute_reply":"2023-12-20T14:50:09.382376Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(24418, 24418, 3055, 3055)"},"metadata":{}}]},{"cell_type":"code","source":"train = pd.DataFrame()\ntrain['Word'] = train_words\ntrain['Pos'] = train_labels\ntrain.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:09.385069Z","iopub.execute_input":"2023-12-20T14:50:09.385425Z","iopub.status.idle":"2023-12-20T14:50:09.437847Z","shell.execute_reply.started":"2023-12-20T14:50:09.385391Z","shell.execute_reply":"2023-12-20T14:50:09.436875Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                                Word  \\\n0  [Ndiyabazi, abalandeli, bethu, balambele, nton...   \n1  [Ndiyayazi, nendlela, abaziva, ngayo, ngento, ...   \n\n                                           Pos  \n0        [VERB, NOUN, PRON, VERB, NOUN, PUNCT]  \n1  [VERB, NOUN, VERB, PRON, NOUN, VERB, PUNCT]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Pos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[Ndiyabazi, abalandeli, bethu, balambele, nton...</td>\n      <td>[VERB, NOUN, PRON, VERB, NOUN, PUNCT]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[Ndiyayazi, nendlela, abaziva, ngayo, ngento, ...</td>\n      <td>[VERB, NOUN, VERB, PRON, NOUN, VERB, PUNCT]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"valid = pd.DataFrame()\nvalid['Word'] = valid_words\nvalid['Pos'] = valid_labels\nvalid.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:09.439068Z","iopub.execute_input":"2023-12-20T14:50:09.439428Z","iopub.status.idle":"2023-12-20T14:50:09.458225Z","shell.execute_reply.started":"2023-12-20T14:50:09.439398Z","shell.execute_reply":"2023-12-20T14:50:09.457118Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                                Word  \\\n0  [Mapurisa, e, Zimbabwe, Republic, Police, akap...   \n1  [Mapurisa, akaparura, chirongwa, ichi, ne, Chi...   \n\n                                                 Pos  \n0  [NOUN, ADP, PROPN, ADJ, NOUN, VERB, NOUN, ADJ,...  \n1  [NOUN, VERB, NOUN, DET, ADP, NOUN, VERB, ADP, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Pos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[Mapurisa, e, Zimbabwe, Republic, Police, akap...</td>\n      <td>[NOUN, ADP, PROPN, ADJ, NOUN, VERB, NOUN, ADJ,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[Mapurisa, akaparura, chirongwa, ichi, ne, Chi...</td>\n      <td>[NOUN, VERB, NOUN, DET, ADP, NOUN, VERB, ADP, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(train.shape, valid.shape, test.shape)\n\nlabels = [\"X\", \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\"]\nCFG.num_classes = len(labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:09.459654Z","iopub.execute_input":"2023-12-20T14:50:09.460006Z","iopub.status.idle":"2023-12-20T14:50:09.468461Z","shell.execute_reply.started":"2023-12-20T14:50:09.459978Z","shell.execute_reply":"2023-12-20T14:50:09.467552Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(24418, 2) (3055, 2) (1208, 5)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Tokenization\n* The code starts by importing the AutoTokenizer class from the \"transformers\" library. This library is commonly used for working with pre-trained models for natural language processing.\n\n* It initializes a tokenizer object using the AutoTokenizer.from_pretrained method. The specific model used for tokenization is determined by the value of CFG.model_nm.\n\n* The convert_to_feature function takes several arguments, including row, which is expected to be a row of data containing words and their corresponding labels. It also takes various configuration parameters, such as max_seq_length, cls_token_at_end, and token-related settings.\n\n* Inside the function, a label_map is created, which is a dictionary that maps labels to their integer representations. This mapping is based on the label_list provided as an argument.\n\n* The function processes the words and labels in the input row. It tokenizes each word using the tokenizer and appends the resulting tokens to the tokens list. The corresponding label IDs are also appended to the label_ids list. If a word is tokenized into multiple sub-tokens, the label is repeated for each sub-token.\n\n* The code accounts for the maximum sequence length by truncating or padding the tokens and label_ids lists. If the length of tokens exceeds max_seq_length minus some special tokens count (which depends on the tokenizer and settings), it truncates the lists.\n\n* It adds special tokens like [SEP] (separator token) and [CLS] (classification token) to the tokens and label_ids lists as needed. The specific position of these tokens depends on the cls_token_at_end setting.\n\n* Segment IDs are created to distinguish between different segments in the input. For most models, sequence_a_segment_id is assigned to all tokens in the input.\n\n* The input_ids are generated by converting the tokens into their corresponding token IDs using the tokenizer.\n\n* An input_mask is created to distinguish between real tokens and padding tokens. Real tokens have a value of 1, while padding tokens have a value of 0.\n\n* The code ensures that the input data is zero-padded to the max_seq_length. The specific padding strategy depends on the pad_on_left setting.\n\n* Several assertions are used to verify the lengths of various lists to ensure they match the max_seq_length.\n\n* Finally, the function returns a dictionary containing the following keys: input_ids, input_mask, segment_ids, and label_ids, which represent the processed input features for a natural language processing model.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n    \ntokenizer = AutoTokenizer.from_pretrained(CFG.model_nm, use_fast=True)\ndef convert_to_feature(\n    row,\n    tokenizer=tokenizer,\n    label_list=labels,\n    max_seq_length=CFG.max_length,\n    cls_token_at_end=False,\n    cls_token=\"[CLS]\",\n    cls_token_segment_id=1,\n    sep_token=\"[SEP]\",\n    sep_token_extra=False,\n    pad_on_left=False,\n    pad_token=0,\n    pad_token_segment_id=0,\n    pad_token_label_id=-100,\n    sequence_a_segment_id=0,\n    mask_padding_with_zero=True,\n):\n    \"\"\" Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n    \"\"\"\n\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    tokens = []\n    label_ids = []\n    for word, label in zip(row['Word'], row['Pos']):\n        word_tokens = tokenizer.tokenize(word)\n        tokens.extend(word_tokens)\n        # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n        label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n\n    # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n    special_tokens_count = 3 if sep_token_extra else 2\n    if len(tokens) > max_seq_length - special_tokens_count:\n        tokens = tokens[: (max_seq_length - special_tokens_count)]\n        label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n\n    tokens += [sep_token]\n    label_ids += [pad_token_label_id]\n    if sep_token_extra:\n        # roberta uses an extra separator b/w pairs of sentences\n        tokens += [sep_token]\n        label_ids += [pad_token_label_id]\n    segment_ids = [sequence_a_segment_id] * len(tokens)\n\n    if cls_token_at_end:\n        tokens += [cls_token]\n        label_ids += [pad_token_label_id]\n        segment_ids += [cls_token_segment_id]\n    else:\n        tokens = [cls_token] + tokens\n        label_ids = [pad_token_label_id] + label_ids\n        segment_ids = [cls_token_segment_id] + segment_ids\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens.\n    # Only real tokens are attended to.\n    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    padding_length = max_seq_length - len(input_ids)\n    if pad_on_left:\n        input_ids = ([pad_token] * padding_length) + input_ids\n        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n        label_ids = ([pad_token_label_id] * padding_length) + label_ids\n    else:\n        input_ids += [pad_token] * padding_length\n        input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n        segment_ids += [pad_token_segment_id] * padding_length\n        label_ids += [pad_token_label_id] * padding_length\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    return dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:09.469921Z","iopub.execute_input":"2023-12-20T14:50:09.470700Z","iopub.status.idle":"2023-12-20T14:50:13.095393Z","shell.execute_reply.started":"2023-12-20T14:50:09.470665Z","shell.execute_reply":"2023-12-20T14:50:13.094401Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7580213b7a1345b49aff8c07d101157e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3801ab3069b443738e298909ca9a4657"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faa23719f39542bc82025cc0c1f78c0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1300c0325a27438f9a87e61e3ec65d34"}},"metadata":{}}]},{"cell_type":"code","source":"%%time\n# train features\ntrain_features = pd.DataFrame(train.apply(lambda row: convert_to_feature(row), axis=1).tolist())\ntrain_features.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:13.096863Z","iopub.execute_input":"2023-12-20T14:50:13.097179Z","iopub.status.idle":"2023-12-20T14:50:46.025054Z","shell.execute_reply.started":"2023-12-20T14:50:13.097142Z","shell.execute_reply":"2023-12-20T14:50:46.023824Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"CPU times: user 32.8 s, sys: 96.6 ms, total: 32.9 s\nWall time: 32.9 s\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                           input_ids  \\\n0  [3, 541, 49778, 159236, 10, 87518, 34642, 186,...   \n1  [3, 99501, 53187, 708, 108, 71065, 10, 159236,...   \n\n                                          input_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                         segment_ids  \\\n0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                           label_ids  \n0  [-100, 16, -100, -100, 8, -100, -100, 11, -100...  \n1  [-100, 16, -100, -100, 8, -100, 16, -100, -100...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>input_mask</th>\n      <th>segment_ids</th>\n      <th>label_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[3, 541, 49778, 159236, 10, 87518, 34642, 186,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 16, -100, -100, 8, -100, -100, 11, -100...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3, 99501, 53187, 708, 108, 71065, 10, 159236,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 16, -100, -100, 8, -100, 16, -100, -100...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n# valid features\nvalid_features = pd.DataFrame(valid.apply(lambda row: convert_to_feature(row), axis=1).tolist())\nvalid_features.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:46.026921Z","iopub.execute_input":"2023-12-20T14:50:46.027433Z","iopub.status.idle":"2023-12-20T14:50:50.762846Z","shell.execute_reply.started":"2023-12-20T14:50:46.027372Z","shell.execute_reply":"2023-12-20T14:50:50.761709Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"CPU times: user 4.7 s, sys: 12 ms, total: 4.71 s\nWall time: 4.71 s\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                           input_ids  \\\n0  [3, 911, 51166, 433, 28, 147326, 47806, 72110,...   \n1  [3, 911, 51166, 433, 15623, 2500, 3168, 1658, ...   \n\n                                          input_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                         segment_ids  \\\n0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                           label_ids  \n0  [-100, 8, -100, -100, 2, 12, 1, 8, 16, -100, -...  \n1  [-100, 8, -100, -100, 16, -100, -100, 8, -100,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>input_mask</th>\n      <th>segment_ids</th>\n      <th>label_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[3, 911, 51166, 433, 28, 147326, 47806, 72110,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 8, -100, -100, 2, 12, 1, 8, 16, -100, -...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3, 911, 51166, 433, 15623, 2500, 3168, 1658, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 8, -100, -100, 16, -100, -100, 8, -100,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n# test features\ntest_features = pd.DataFrame(test.apply(lambda row: convert_to_feature(row), axis=1).tolist())\ntest_features.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:50.764067Z","iopub.execute_input":"2023-12-20T14:50:50.764391Z","iopub.status.idle":"2023-12-20T14:50:52.622704Z","shell.execute_reply.started":"2023-12-20T14:50:50.764337Z","shell.execute_reply":"2023-12-20T14:50:52.621416Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"CPU times: user 1.83 s, sys: 8.94 ms, total: 1.84 s\nWall time: 1.83 s\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                           input_ids  \\\n0  [3, 799, 36, 5083, 5551, 170, 28, 8562, 61783,...   \n1  [3, 602, 95635, 108, 36, 658, 3613, 741, 299, ...   \n\n                                          input_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                         segment_ids  \\\n0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                           label_ids  \n0  [-100, 0, 0, -100, 0, -100, 0, 0, 0, 0, 0, 0, ...  \n1  [-100, 0, -100, 0, 0, -100, -100, 0, -100, -10...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>input_mask</th>\n      <th>segment_ids</th>\n      <th>label_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[3, 799, 36, 5083, 5551, 170, 28, 8562, 61783,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 0, 0, -100, 0, -100, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3, 602, 95635, 108, 36, 658, 3613, 741, 299, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 0, -100, 0, 0, -100, -100, 0, -100, -10...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"masakhane = DatasetDict({\n    \"train\": Dataset.from_pandas(train_features),\n    \"valid\": Dataset.from_pandas(valid_features),\n    \"test\": Dataset.from_pandas(test_features),\n})\n\nmasakhane","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:52.624270Z","iopub.execute_input":"2023-12-20T14:50:52.624723Z","iopub.status.idle":"2023-12-20T14:50:54.157002Z","shell.execute_reply.started":"2023-12-20T14:50:52.624690Z","shell.execute_reply":"2023-12-20T14:50:54.155130Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'input_mask', 'segment_ids', 'label_ids'],\n        num_rows: 24418\n    })\n    valid: Dataset({\n        features: ['input_ids', 'input_mask', 'segment_ids', 'label_ids'],\n        num_rows: 3055\n    })\n    test: Dataset({\n        features: ['input_ids', 'input_mask', 'segment_ids', 'label_ids'],\n        num_rows: 1208\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"label_map = {i: label for i, label in enumerate(labels)}","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:54.158296Z","iopub.execute_input":"2023-12-20T14:50:54.158647Z","iopub.status.idle":"2023-12-20T14:50:54.166785Z","shell.execute_reply.started":"2023-12-20T14:50:54.158610Z","shell.execute_reply":"2023-12-20T14:50:54.165937Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Metrics","metadata":{}},{"cell_type":"code","source":"def compute_metrics(\n    eval_pred,\n    pad_token_label_id=-100,\n):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis = -1)\n\n    out_label_list = []\n    preds_list = []\n    for i in range(labels.shape[0]):\n        for j in range(labels.shape[1]):\n            if labels[i, j] != pad_token_label_id:\n                out_label_list.append(label_map[labels[i][j]])\n                preds_list.append(label_map[preds[i][j]])\n\n    accuracy = accuracy_score(out_label_list, preds_list)\n    return {\"accuracy\": accuracy}","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:54.168165Z","iopub.execute_input":"2023-12-20T14:50:54.168565Z","iopub.status.idle":"2023-12-20T14:50:54.180314Z","shell.execute_reply.started":"2023-12-20T14:50:54.168524Z","shell.execute_reply":"2023-12-20T14:50:54.179391Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Build The Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\nmodel = AutoModelForTokenClassification.from_pretrained(\n    CFG.model_nm,\n    num_labels = CFG.num_classes,\n    id2label={str(i): label for i, label in enumerate(labels)},\n    label2id={label: i for i, label in enumerate(labels)},\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:50:54.182160Z","iopub.execute_input":"2023-12-20T14:50:54.182529Z","iopub.status.idle":"2023-12-20T14:53:54.491040Z","shell.execute_reply.started":"2023-12-20T14:50:54.182498Z","shell.execute_reply":"2023-12-20T14:53:54.490037Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/714 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bedb4042ae94f738f1145cb4eeb2c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dc896f5dde345638821a1eebbb4248c"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"epoch_steps = int(np.ceil(len(masakhane['train']) / CFG.batch_size))\ndisplay(epoch_steps)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:53:54.492271Z","iopub.execute_input":"2023-12-20T14:53:54.492571Z","iopub.status.idle":"2023-12-20T14:53:54.500582Z","shell.execute_reply.started":"2023-12-20T14:53:54.492544Z","shell.execute_reply":"2023-12-20T14:53:54.499635Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"1527"},"metadata":{}}]},{"cell_type":"markdown","source":"### Training and Evaluation","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./masakhane-pos',\n    evaluation_strategy = \"steps\",\n    save_strategy='steps',\n    eval_steps = epoch_steps//2,\n    save_steps = epoch_steps//2,\n    logging_strategy=\"steps\",\n    logging_steps= epoch_steps//2,\n    learning_rate=CFG.lr,\n    save_total_limit=1,\n    num_train_epochs=CFG.num_epoch,\n    fp16=True,\n    report_to='none',\n    metric_for_best_model = \"accuracy\",\n    greater_is_better=True,\n    load_best_model_at_end = True,\n    per_device_train_batch_size=CFG.batch_size,\n    per_device_eval_batch_size=CFG.batch_size,\n    warmup_steps = CFG.warmup_steps,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:53:54.501978Z","iopub.execute_input":"2023-12-20T14:53:54.502830Z","iopub.status.idle":"2023-12-20T14:53:54.512811Z","shell.execute_reply.started":"2023-12-20T14:53:54.502803Z","shell.execute_reply":"2023-12-20T14:53:54.511829Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model = model,\n    args = training_args,\n    compute_metrics = compute_metrics,\n    train_dataset = masakhane['train'],\n    eval_dataset = masakhane['valid'],\n    tokenizer = tokenizer,\n    callbacks = [EarlyStoppingCallback(5)],\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:53:54.519358Z","iopub.execute_input":"2023-12-20T14:53:54.519654Z","iopub.status.idle":"2023-12-20T14:53:57.568818Z","shell.execute_reply.started":"2023-12-20T14:53:54.519618Z","shell.execute_reply":"2023-12-20T14:53:57.567711Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-20T14:53:57.570378Z","iopub.execute_input":"2023-12-20T14:53:57.570791Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6042' max='22920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 6042/22920 3:22:49 < 9:26:47, 0.50 it/s, Epoch 7.91/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>763</td>\n      <td>1.777400</td>\n      <td>1.046259</td>\n      <td>0.706392</td>\n    </tr>\n    <tr>\n      <td>1526</td>\n      <td>0.878600</td>\n      <td>0.845609</td>\n      <td>0.761507</td>\n    </tr>\n    <tr>\n      <td>2289</td>\n      <td>0.677500</td>\n      <td>0.804041</td>\n      <td>0.774852</td>\n    </tr>\n    <tr>\n      <td>3052</td>\n      <td>0.581400</td>\n      <td>0.781334</td>\n      <td>0.779300</td>\n    </tr>\n    <tr>\n      <td>3815</td>\n      <td>0.526200</td>\n      <td>0.779480</td>\n      <td>0.777291</td>\n    </tr>\n    <tr>\n      <td>4578</td>\n      <td>0.488200</td>\n      <td>0.777088</td>\n      <td>0.776538</td>\n    </tr>\n    <tr>\n      <td>5341</td>\n      <td>0.456400</td>\n      <td>0.758541</td>\n      <td>0.778332</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save Your Model To The Hub","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\n\nfolder_path = \"/kaggle/working/masakhane-pos\"  # Replace with the path to your folder\n\n# Remove existing files (if you don't need them)\nshutil.rmtree(folder_path, ignore_errors=True)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.push_to_hub(\"Koleshjr/masakhane-pos\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"loaded_model = AutoModelForTokenClassification.from_pretrained(\n    \"Koleshjr/masakhane-pos\",\n    num_labels = CFG.num_classes,\n\n)\n\nloaded_tokenizer = AutoTokenizer.from_pretrained(\n    'Koleshjr/masakhane-pos'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model, trainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pad_token_label_id=-100\n\ntest_args = TrainingArguments(\n    output_dir= '/content/',\n    do_train =False,\n    do_predict = True,\n    dataloader_drop_last = False\n)\n\ntrainer = Trainer(\n    model = loaded_model,\n    args = test_args,\n)\n\ntest_results = trainer.predict(masakhane['test'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = test_results.predictions.argmax(axis = -1)\n\npreds_list = [[] for _ in range(test_results.label_ids.shape[0])]\nfor i in range(test_results.label_ids.shape[0]):\n    for j in range(test_results.label_ids.shape[1]):\n        if test_results.label_ids[i, j] != pad_token_label_id:\n            preds_list[i].append(label_map[result[i][j]])\n\ntest['Pos'] = preds_list\ntest.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_rows_with_mismatched_lengths(df):\n    for index, row in df.iterrows():\n        if len(row['Id']) != len(row['Pos']):\n            print(f\"Row {index}: 'Id' length = {len(row['Id'])}, 'Pos' length = {len(row['Pos'])} sentence_id: {row['sentence_Id']}\")\n\n# Call the function to print rows with mismatched lengths\nprint_rows_with_mismatched_lengths(test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def add_nouns_to_match_lengths(df):\n#     for index, row in df.iterrows():\n#         id_length = len(row['Id'])\n#         pos_length = len(row['Pos'])\n#         if id_length > pos_length:\n#             # Calculate the difference in lengths\n#             diff = id_length - pos_length\n#             # Add 'NOUN' to 'Pos' to match lengths\n#             df.at[index, 'Pos'].extend(['NOUN'] * diff)\n\n# # Call the function to add 'NOUN' labels to match lengths\n# add_nouns_to_match_lengths(test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test[['Id', 'Pos']].explode(column=['Id', 'Pos'], ignore_index=True)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = submission.reset_index(drop=True)\nsubmission.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"experiment_1.csv\", index = False)\nsubmission['Pos'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}